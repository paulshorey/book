# Tokenization / Lemmatization

* **tokenization** is breaking up a large text or data into smaller morphemes/lexemes/tokens
* **stemming** is the process of reducing inflected \(or sometimes derived\) words to their [word stem](https://en.wikipedia.org/wiki/Word_stem), base or [root](https://en.wikipedia.org/wiki/Root_%28linguistics%29) form
* **lemmatization** is like stemming, but with knowledge of context _The word "better" has "good" as its lemma. This link is missed by stemming, as it requires a dictionary look-up._

Good discussion:  
[https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words](https://stackoverflow.com/questions/8870261/how-to-split-text-without-spaces-into-list-of-words)

**This is great:**  
[https://github.com/keredson/wordninja](https://github.com/keredson/wordninja)

**Now to \(for real\) transcribe python to javascript:**  
[https://www.infoworld.com/article/3209651/how-to-convert-python-to-javascript-and-back-again.html](https://www.infoworld.com/article/3209651/how-to-convert-python-to-javascript-and-back-again.html)  
****[**https://github.com/metapensiero/metapensiero.pj**](https://github.com/metapensiero/metapensiero.pj) **&lt;--**

Upgrade default python:  
[https://opensource.com/article/19/5/python-3-default-mac](https://opensource.com/article/19/5/python-3-default-mac)

